{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalevala 1849\n",
      "\n",
      "Kalevalan ensimmäinen painos ilmestyi vuonna 1835. Teos syntyi Elias Lönnrotin\n",
      "(1802 \n",
      "569255\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "with open('./../filtered_training_data/Finnish/train_data_kalevala.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Print the first 100 characters\n",
    "print(content[:100])\n",
    "\n",
    "# Display the length of the content\n",
    "print(len(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"'(),.01234589:;?@AEHIJKLMNOPRSTUVYadefghijklmnoprstuvyÄäö\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "# Create a set of unique characters\n",
    "chars = sorted(list(set(content)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Display the characters\n",
    "print(''.join(chars))\n",
    "\n",
    "# Display the number of characters\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 38, 47, 40, 56, 38, 47, 38, 1, 10, 15, 13, 16, 0, 0, 26, 38, 47, 40, 56, 38, 47, 38, 49, 1, 40, 49, 53, 44, 48, 48, 59, 44, 49, 40, 49, 1, 51, 38, 44, 49, 50, 53, 1, 44, 47, 48, 40, 53, 54, 57, 44, 1, 56, 55, 50, 49, 49, 38, 1, 10, 15, 12, 14, 8, 1, 34, 40, 50, 53, 1, 53, 57, 49, 54, 57, 44, 1, 22, 47, 44, 38, 53, 1, 27, 60, 49, 49, 52, 50, 54, 44, 49, 0, 5, 10, 15, 9, 11, 1]\n",
      "Kalevala 1849\n",
      "\n",
      "Kalevalan ensimmäinen painos ilmestyi vuonna 1835. Teos syntyi Elias Lönnrotin\n",
      "(1802 \n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary of characters mapped to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encoded = lambda s: [stoi[ch] for ch in s] # encode a string\n",
    "decoded = lambda l: ''.join([itos[i] for i in l]) # decode a list of integers\n",
    "\n",
    "# Display the encoded and decoded versions of the first 100 characters\n",
    "print(encoded(content[:100]))\n",
    "print(decoded(encoded(content[:100])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([26, 38, 47,  ..., 53, 38,  8])\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire content\n",
    "import torch\n",
    "\n",
    "encoded_content = encoded(content)\n",
    "encoded_content = torch.tensor(encoded_content, dtype=torch.int64)\n",
    "\n",
    "# Display the encoded content\n",
    "print(encoded_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0\n",
      "Max: 60\n",
      "Mean: 38.12738037109375\n",
      "Std: 19.195058822631836\n",
      "Length: 569255\n"
     ]
    }
   ],
   "source": [
    "# Display some statistics about the encoded content\n",
    "print('Min:', encoded_content.min().item())\n",
    "print('Max:', encoded_content.max().item())\n",
    "print('Mean:', encoded_content.float().mean().item())\n",
    "print('Std:', encoded_content.float().std().item())\n",
    "print('Length:', len(encoded_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_size = int(len(encoded_content) * 0.8)\n",
    "train_data = encoded_content[:train_size]\n",
    "val_data = encoded_content[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n",
      "tensor([[56, 44, 47,  ..., 38, 43, 38],\n",
      "        [49,  7,  0,  ..., 56, 44, 54],\n",
      "        [43, 57, 56,  ..., 50, 46, 53],\n",
      "        ...,\n",
      "        [51, 44, 50,  ..., 44,  1, 56],\n",
      "        [55, 53,  7,  ..., 56, 38, 47],\n",
      "        [27, 40, 49,  ..., 49,  1, 53]])\n",
      "torch.Size([32, 128])\n",
      "tensor([[44, 47, 44,  ..., 43, 38, 49],\n",
      "        [ 7,  0,  1,  ..., 44, 54, 54],\n",
      "        [57, 56, 59,  ..., 46, 53, 44],\n",
      "        ...,\n",
      "        [44, 50, 49,  ...,  1, 56, 59],\n",
      "        [53,  7,  0,  ..., 38, 47, 45],\n",
      "        [40, 49, 54,  ...,  1, 53, 38]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 32 # number of sequences processed in parallel\n",
    "block_size = 128 # max context length\n",
    "\n",
    "# Create a function to generate batches of data\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i: i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1: i + 1 + block_size] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# Display the shapes of the first batch\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an example of the first batch\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t]\n",
    "        target = yb[b, t]\n",
    "        # print(f'Context: {context}')\n",
    "        # print(f'Target: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 61])\n",
      "tensor(4.6249, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "0A1J@@Ti,faOLUIdTeP.1K)oi;U:a.hKUHgMeo03MmUHP@2@\"9luAKNYTiEmNy8Ä)4TäMh3uS0s8SKLh8ÄYa,S@\n",
      "yöoK2PEkujuj,f2 \"(ÄLpH8ÄölKf3äI8g,'t?iO\";'aHLRöLP.NR0ytSKpfnN2ÄtiO?(\";ldÄÄ,yR,dNg83\n",
      "5i1pni5KTiL3ähP h(2OÄ94y?esniÄOdaa1 AäU;ekHM?8gaö;äU3ukMn\n",
      "9K29nR,\n",
      "3l8,n 2gMtnRMYö14y8sJhY.MaOöJsAao8PP4nHs,o5\n",
      "jä45ÄdeTuOSkp\"MSOPSKV\"8(vMh3@H3tvL\n",
      "VK,)4)4röujt4gHLi;'rjT\"e05uO)(O\n",
      "S(?Tetiu9pvE5(8:\n",
      " (tvvOOti!!ilHU5EmN2j4gg)sla\n",
      "KsNA!nR?2mtö:d0;3tiu(uV0öuO!HsfKr.!sSMeoaII1@mm,nHOOj !) Tu!jAöAKAU)ju;äö8,Yr84fRv\"VRkSH4ÄYja)jSpumöujhUHrKj0\"gldU\"fÄ:3nR2ÄM(eäU?f@0i,KpM9K2Uptö4EK;,oSpkv2;YJI?i,3ä@rYoeo5aNhO2L3LoRf\"y''dT8MO!Sp)('gOeR4V\"es\"0@dHUa4'HÄaOuJJdV\";m1,4VAdTf2OkmaR5@rUH3LoueM18ÄÄOSAäsEm?ÄyäÄL50,V')vök;ökRäu!SÄMgMu(g@,8 (dväETAH(I i5,g02V(diSä:3Pyj.g3Oö4o@AhKLiudrj0TuNÄUj,sff!\n",
      "4V9@khyYp:R!S imi5.eYg)j1ö:0v14öd)R5uKr..äjKr S4jtiIAdU;YnJI@rTp08AHk.@3Li!!sjNdUVA3Lmlans.eT5vOnsE:3u2rP.äoMgET8d0uO 'y8V;lP5vp4UNnR2O4p\n",
      "IöL\n",
      "Kj5K@rjäILiÄd?8e0T84:9jKS5@la5Ä@!4TeTHYSaM.i;@k!''TRVM)oU;JaP\",,RJKOöTsPETV SKOL3Lf9n1yf89KNi,äpkRhÄI8i1@rJI\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(-1) # B*T\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_tokens=100):\n",
    "        for _ in range(max_tokens):\n",
    "            logits, loss = self(idx) # predictions\n",
    "            logits = logits[:, -1, :] # last prediction\n",
    "            probs = F.softmax(logits, dim=1) # probabilities\n",
    "            idx_new = torch.multinomial(probs, num_samples=1) # new index\n",
    "            idx = torch.cat([idx, idx_new], dim=1) # add to the sequence\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.int64)\n",
    "print(decoded(m.generate(idx, max_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10000/10000 [00:23<00:00, 424.34step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3332417011260986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch optimizer\n",
    "opt = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the model\n",
    "for steps in tqdm(range(10000), desc=\"Training\", unit=\"step\"):\n",
    "    xb, yb = get_batch('train') # get a batch of data\n",
    "    logits, loss = m(xb, yb)\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "print(f'Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " kstajataissiki usunuttojasatänt mahoutäinnän,\n",
      " va,\n",
      " akit yvaaummevilluon hei,\n",
      " staulkoreisinutonelmalotä hamikahinelöykstsen oaiese' atyvälka, seta,\n",
      " kkinan Mana jä \"Läillesa,\n",
      " \"Ma kan vi ovelika pa etäpiren keropytaja vi.\n",
      " m9llon vöertä harola,\n",
      "\n",
      " yNä' va.\n",
      "  jaulangelallu Vise panei.\n",
      " t:\n",
      " o5tosurjahalÄsenent lauisetä n umäsäimäse llarrtan Sa,\n",
      " ken, evai:\n",
      " ha llelain,\n",
      " hata,\n",
      " kuui:\n",
      " Vellevisehomikatt mäna,\n",
      " mestäpihtsauulusasitisttesa,\n",
      " pokonrvojoikän pämen psen vavare vatin vot on kaienetunnsä isi:\n",
      " en si roittä sstsuotitikamevä,\n",
      " eni:\n",
      " lÄä, kitema,\n",
      " lläihina;\n",
      " Uni sesttervalehjasanäää.\n",
      " loino ppähatäinelele,\n",
      " lmisoaika llhtei,\n",
      " lltena,\n",
      " lemeran alPo se Sisusihtantssise Katä \"Oi,\n",
      " pi,\n",
      " llituahitskon lliksuoikeluovähaleniri!\n",
      " vanahaiveroi ta,\n",
      " pä pitkeneup'ohoisisteitävi kuta?\n",
      " La visäisinärerjoni mähine aahi vama,\n",
      " Loisyn aap'isimine va vin, Kytäisi ke väsuosehätetoinuhehessehotyöyyöirrmökonehahetttvisima lilapin\n",
      " Aja.\n",
      " tän petssujahana, kippo ta mmpipiltisapä,\n",
      " tut kuokullluakarella \n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.int64)\n",
    "print(decoded(m.generate(idx, max_tokens=1000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-creation-MVIDSWk0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
